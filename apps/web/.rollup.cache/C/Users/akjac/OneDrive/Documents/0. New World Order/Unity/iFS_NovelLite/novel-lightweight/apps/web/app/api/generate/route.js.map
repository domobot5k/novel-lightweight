{"version":3,"file":"route.js","sourceRoot":"","sources":["route.ts"],"names":[],"mappings":"AAAA,OAAO,MAAM,MAAM,QAAQ,CAAC;AAC5B,OAAO,EAAE,YAAY,EAAE,qBAAqB,EAAE,MAAM,IAAI,CAAC;AACzD,OAAO,EAAE,EAAE,EAAE,MAAM,YAAY,CAAC;AAChC,OAAO,EAAE,SAAS,EAAE,MAAM,oBAAoB,CAAC;AAE/C,sDAAsD;AACtD,MAAM,MAAM,GAAG,IAAI,MAAM,CAAC;IACxB,MAAM,EAAE,OAAO,CAAC,GAAG,CAAC,cAAc,IAAI,EAAE;CACzC,CAAC,CAAC;AAEH,oGAAoG;AACpG,MAAM,CAAC,MAAM,OAAO,GAAG,MAAM,CAAC;AAE9B,MAAM,CAAC,KAAK,UAAU,IAAI,CAAC,GAAY;IACrC,wDAAwD;IACxD,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,cAAc,IAAI,OAAO,CAAC,GAAG,CAAC,cAAc,KAAK,EAAE,EAAE;QACpE,OAAO,IAAI,QAAQ,CACjB,iEAAiE,EACjE;YACE,MAAM,EAAE,GAAG;SACZ,CACF,CAAC;KACH;IACD,IACE,OAAO,CAAC,GAAG,CAAC,QAAQ,IAAI,aAAa;QACrC,OAAO,CAAC,GAAG,CAAC,eAAe;QAC3B,OAAO,CAAC,GAAG,CAAC,iBAAiB,EAC7B;QACA,MAAM,EAAE,GAAG,GAAG,CAAC,OAAO,CAAC,GAAG,CAAC,iBAAiB,CAAC,CAAC;QAC9C,MAAM,SAAS,GAAG,IAAI,SAAS,CAAC;YAC9B,KAAK,EAAE,EAAE;YACT,OAAO,EAAE,SAAS,CAAC,aAAa,CAAC,EAAE,EAAE,KAAK,CAAC;SAC5C,CAAC,CAAC;QAEH,MAAM,EAAE,OAAO,EAAE,KAAK,EAAE,KAAK,EAAE,SAAS,EAAE,GAAG,MAAM,SAAS,CAAC,KAAK,CAChE,mBAAmB,EAAE,EAAE,CACxB,CAAC;QAEF,IAAI,CAAC,OAAO,EAAE;YACZ,OAAO,IAAI,QAAQ,CAAC,kDAAkD,EAAE;gBACtE,MAAM,EAAE,GAAG;gBACX,OAAO,EAAE;oBACP,mBAAmB,EAAE,KAAK,CAAC,QAAQ,EAAE;oBACrC,uBAAuB,EAAE,SAAS,CAAC,QAAQ,EAAE;oBAC7C,mBAAmB,EAAE,KAAK,CAAC,QAAQ,EAAE;iBACtC;aACF,CAAC,CAAC;SACJ;KACF;IAED,IAAI,EAAE,MAAM,EAAE,GAAG,MAAM,GAAG,CAAC,IAAI,EAAE,CAAC;IAElC,MAAM,QAAQ,GAAG,MAAM,MAAM,CAAC,IAAI,CAAC,WAAW,CAAC,MAAM,CAAC;QACpD,KAAK,EAAE,eAAe;QACtB,QAAQ,EAAE;YACR;gBACE,IAAI,EAAE,QAAQ;gBACd,OAAO,EACL,iGAAiG;oBACjG,6EAA6E;oBAC7E,oGAAoG;gBACtG,mKAAmK;gBACnK,+CAA+C;aAChD;YACD;gBACE,IAAI,EAAE,MAAM;gBACZ,OAAO,EAAE,MAAM;aAChB;SACF;QACD,WAAW,EAAE,GAAG;QAChB,KAAK,EAAE,CAAC;QACR,iBAAiB,EAAE,CAAC;QACpB,gBAAgB,EAAE,CAAC;QACnB,MAAM,EAAE,IAAI;QACZ,CAAC,EAAE,CAAC;KACL,CAAC,CAAC;IAEH,mDAAmD;IACnD,MAAM,MAAM,GAAG,YAAY,CAAC,QAAQ,CAAC,CAAC;IAEtC,0BAA0B;IAC1B,OAAO,IAAI,qBAAqB,CAAC,MAAM,CAAC,CAAC;AAC3C,CAAC","sourcesContent":["import OpenAI from \"openai\";\r\nimport { OpenAIStream, StreamingTextResponse } from \"ai\";\r\nimport { kv } from \"@vercel/kv\";\r\nimport { Ratelimit } from \"@upstash/ratelimit\";\r\n\r\n// Create an OpenAI API client (that's edge friendly!)\r\nconst openai = new OpenAI({\r\n  apiKey: process.env.OPENAI_API_KEY || \"\",\r\n});\r\n\r\n// IMPORTANT! Set the runtime to edge: https://vercel.com/docs/functions/edge-functions/edge-runtime\r\nexport const runtime = \"edge\";\r\n\r\nexport async function POST(req: Request): Promise<Response> {\r\n  // Check if the OPENAI_API_KEY is set, if not return 400\r\n  if (!process.env.OPENAI_API_KEY || process.env.OPENAI_API_KEY === \"\") {\r\n    return new Response(\r\n      \"Missing OPENAI_API_KEY – make sure to add it to your .env file.\",\r\n      {\r\n        status: 400,\r\n      },\r\n    );\r\n  }\r\n  if (\r\n    process.env.NODE_ENV != \"development\" &&\r\n    process.env.KV_REST_API_URL &&\r\n    process.env.KV_REST_API_TOKEN\r\n  ) {\r\n    const ip = req.headers.get(\"x-forwarded-for\");\r\n    const ratelimit = new Ratelimit({\r\n      redis: kv,\r\n      limiter: Ratelimit.slidingWindow(50, \"1 d\"),\r\n    });\r\n\r\n    const { success, limit, reset, remaining } = await ratelimit.limit(\r\n      `novel_ratelimit_${ip}`,\r\n    );\r\n\r\n    if (!success) {\r\n      return new Response(\"You have reached your request limit for the day.\", {\r\n        status: 429,\r\n        headers: {\r\n          \"X-RateLimit-Limit\": limit.toString(),\r\n          \"X-RateLimit-Remaining\": remaining.toString(),\r\n          \"X-RateLimit-Reset\": reset.toString(),\r\n        },\r\n      });\r\n    }\r\n  }\r\n\r\n  let { prompt } = await req.json();\r\n\r\n  const response = await openai.chat.completions.create({\r\n    model: \"gpt-3.5-turbo\",\r\n    messages: [\r\n      {\r\n        role: \"system\",\r\n        content:\r\n          \"You are an AI writing assistant that continues existing text based on context from prior text. \" +\r\n          \"Give more weight/priority to the later characters than the beginning ones. \" +\r\n          \"Limit your response to no more than 200 characters, but make sure to construct complete sentences.\",\r\n        // we're disabling markdown for now until we can figure out a way to stream markdown text with proper formatting: https://github.com/steven-tey/novel/discussions/7\r\n        // \"Use Markdown formatting when appropriate.\",\r\n      },\r\n      {\r\n        role: \"user\",\r\n        content: prompt,\r\n      },\r\n    ],\r\n    temperature: 0.7,\r\n    top_p: 1,\r\n    frequency_penalty: 0,\r\n    presence_penalty: 0,\r\n    stream: true,\r\n    n: 1,\r\n  });\r\n\r\n  // Convert the response into a friendly text-stream\r\n  const stream = OpenAIStream(response);\r\n\r\n  // Respond with the stream\r\n  return new StreamingTextResponse(stream);\r\n}\r\n"]}